Weekly Report by Yitu_3/19/2019
===================
I found some recent popular topics such as computing in/near memory, were mentioned in the tutrotial for DLA of MIT. Therefore, I read <Efficient Processing of Deep Neural Networks: A Tutorial and Survey> again and took some notes. Afterthat, I read a paper of HPCA 2019 in detail, <Hypar: Towards Hybrid Parallelism for Deep Learning Accelerator Array>, which focuses on the model parallelsim and data parallelism for the level of accelerator and the process of trainig rather than just focusing on the data parrallelsim for computation 
for inference. It is novel and fresh because few papers work on designing architecture for the process of training.

The detailed notes for the two papers and the papers themselves mentioned above have been uploaded in the form of powerpoints in the Weekly Reports_3-19-2019 folder.
--------------------
