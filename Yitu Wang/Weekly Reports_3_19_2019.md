Weekly Report by Yitu_3/19/2019
===================
I found some recent popular topics such as computing in/near memory, were mentioned in the tutrotial for DLA of MIT. Therefore, I read
<Efficient Processing of Deep Neural Networks: A Tutorial and Survey> again and took some notes. Afterthat, I read a paper of HPCA 2019
in detail, <Hypar: Towards Hybrid Parallelism for Deep Learning Accelerator Array>, which focuses on the model parallelsim and data 
parallelism for the level of accelerator and the process of trainig rather than just focusing on the data parrallelsim for computation 
for inference. It is novel and fresh because few papers work on designing architecture for the process of training.

Here are the notes that I took for the two papers mentioned above.

Notes for Hypar
---------------
